{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29871f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"multiple_linear_regression_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34697cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>experience</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>30450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>35670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>31580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>40130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>47830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  experience  income\n",
       "0   25           1   30450\n",
       "1   30           3   35670\n",
       "2   47           2   31580\n",
       "3   32           5   40130\n",
       "4   43          10   47830"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54493f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'experience', 'income'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65fc2225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ed0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which columns are inputs?\n",
    "#age, experience\n",
    "\n",
    "#Which column is the output?\n",
    "#income\n",
    "\n",
    "#How many features does your model need to handle?\n",
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664f3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[[\"age\", \"experience\"]].values\n",
    "y = df[\"income\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33750248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the shape of X ?\n",
    "#(20, 2)\n",
    "\n",
    "#What is the shape of y ?\n",
    "#(20,)\n",
    "\n",
    "#Why does X have 2 columns but y only one?\n",
    "#X has 2 columns because it contains the input features (age and experience), while y has only one column because it contains the output variable (income) that we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f0f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_features = x.shape[1]\n",
    "w = np.zeros(n_features)\n",
    "b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e0d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why do we need one weight per feature?\n",
    "#We need one weight per feature because each feature contributes differently to the prediction of the output variable. The weights allow us to assign different levels of importance to each feature in the linear regression model.\n",
    "\n",
    "#Why is Bias seperate?\n",
    "#Bias is separate because it allows the model to make predictions even when all input features are zero. It acts as an intercept term that shifts the regression line up or down, enabling the model to fit the data better.\n",
    "\n",
    "#Would initializing with large values be risky?\n",
    "#Yes, because it may lead to slow convergence or divergence during the training process. Large initial weights can cause the model to make large updates to the weights during training, which can result in overshooting the optimal solution and failing to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0440c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    y_hat = np.dot(x, w) + b\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498a632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why is there no activation function?\n",
    "#In linear regression, we are modeling a continuous output variable, so we do not need an activation function. \n",
    "\n",
    "# What kind of values can y_hat take?\n",
    "# The output of the linear regression model can take any real value, which is appropriate for regression tasks. \n",
    "\n",
    "# How is this different from logistic regression?\n",
    "# logistic regression is used for classification tasks where the output is a probability between 0 and 1, and therefore requires an activation function (sigmoid) to map the linear output to a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20a8b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, y_hat):\n",
    "    loss = np.mean((y - y_hat) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e850da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why square the error?\n",
    "#Squaring the error ensures that all errors are positive and gives more weight to larger errors, which can help the model to focus on minimizing larger mistakes during training.\n",
    "\n",
    "#What happens if one prediction is very wrong?\n",
    "#If one prediction is very wrong, it will have a large squared error, which can significantly increase the overall mean squared error. This can make the model more sensitive to outliers and may lead to a less accurate fit for the majority of the data points.\n",
    "\n",
    "#Why not just take the absolute value of the error?\n",
    "#Taking the absolute value of the error (mean absolute error) is another option, but it does not penalize larger errors as much as mean squared error. Mean squared error can be more effective in certain cases where we want to give more importance to larger errors, while mean absolute error can be more robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33d7c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(x, y, y_hat):\n",
    "    n = len(y)\n",
    "    dw = (-2/n) * np.dot(x.T, (y - y_hat))\n",
    "    db = (-2/n) * np.sum(y - y_hat)\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3359118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why does X appear in dw but not in db ?\n",
    "#x appears in dw because the gradient with respect to the weights (dw) depends on the input features, while the gradient with respect to the bias (db) does not depend on the input features and is only influenced by the difference between the true and predicted values.\n",
    "\n",
    "#Why does the error term appear everywhere?\n",
    "#The error term appears everywhere because it represents the difference between the true values and the predicted values, which is the basis for calculating both the loss and the gradients. The model uses this error to adjust the weights and bias in order to minimize the loss during training.\n",
    "\n",
    "#What happens if the error is zero?\n",
    "#It means that the model's predictions perfectly match the true values, resulting in a mean squared error of zero. In this case, the gradients (dw and db) would also be zero, indicating that there is no need to update the weights and bias further, as the model has already achieved optimal performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08565938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w, b, dw, db, lr):\n",
    "    w = w - lr * dw\n",
    "    b = b - lr * db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4247b4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1727049635.0\n",
      "Epoch 100, Loss: 66491868.553113505\n",
      "Epoch 200, Loss: 61752567.2011901\n",
      "Epoch 300, Loss: 58616531.07847047\n",
      "Epoch 400, Loss: 56528801.53951118\n",
      "Epoch 500, Loss: 55126542.02946697\n",
      "Epoch 600, Loss: 54172526.94885705\n",
      "Epoch 700, Loss: 53511656.14292053\n",
      "Epoch 800, Loss: 53042523.72795741\n",
      "Epoch 900, Loss: 52698829.56325033\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    y_hat = predict(x, w, b)\n",
    "    loss = mean_squared_error(y, y_hat)\n",
    "    dw, db = compute_gradients(x, y, y_hat)\n",
    "    w, b = update_parameters(w, b, dw, db, lr)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "593d4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does loss decrease over time?\n",
    "#Yes, the loss should decrease over time as the model learns to make better predictions by updating the weights and bias based on the computed gradients. As the model converges towards the optimal parameters, the mean squared error should reduce, indicating that the predictions are getting closer to the true values.\n",
    "\n",
    "#What happens if it increases?\n",
    "#If the loss increases, it may indicate that the learning rate is too high, causing the model to overshoot the optimal parameters during updates. This can lead to divergence and a failure to converge towards a minimum loss. In such cases, it may be necessary to reduce the learning rate or implement techniques like learning rate decay to stabilize training.\n",
    "\n",
    "#How do learning rate and epochs interact?\n",
    "#The learning rate determines the size of the steps taken towards the minimum loss during each update,while the number of epochs determines how many times the model will iterate over the entire training dataset. A higher learning rate may require fewer epochs to converge, but it can also lead to instability if it's too high. Conversely, a lower learning rate may require more epochs to converge but can provide a more stable training process. It's important to find a balance between the learning rate and the number of epochs to ensure effective training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5791f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights: [ 764.75405919 1371.03430441]\n",
      "Final bias: 321.7364117447249\n",
      "Predicted income: 30119.529709500806\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final weights: {w}\")\n",
    "print(f\"Final bias: {b}\")\n",
    "new_candidate = np.array([[30, 5]])\n",
    "predicted_income = predict(new_candidate, w, b)\n",
    "print(f\"Predicted income: {predicted_income[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7344b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the prediction reasonable?\n",
    "#The reasonableness of the prediction depends on the context of the data and the range of incomes in the dataset. If the predicted income falls within a plausible range based on the training data, then it can be considered reasonable. However, without knowing the specific values in the dataset, it's difficult to definitively assess the reasonableness of the prediction.\n",
    "\n",
    "#Does it interpolate smoothly?\n",
    "#Since the model is a linear regression, it will interpolate smoothly between the training data points. The predicted income for a 30-year-old with 5 years of experience will be a linear combination of the weights and bias learned from the training data, which allows for smooth interpolation.\n",
    "\n",
    "#Why is this better than threshold rules?\n",
    "#Linear regression provides a continuous output, which allows for more nuanced predictions compared to threshold rules that classify inputs into discrete categories. This can be particularly beneficial when the relationship between the input features and the output variable is not strictly binary, allowing for more accurate modeling of real-world data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
