{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd61e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AND = [\n",
    "    (0,0,0),\n",
    "    (0,1,0),\n",
    "    (1,0,0),\n",
    "    (1,1,1)\n",
    "]\n",
    "\n",
    "OR = [\n",
    "    (0,0,0),\n",
    "    (0,1,1),\n",
    "    (1,0,1),\n",
    "    (1,1,1)\n",
    "]\n",
    "\n",
    "NAND = [\n",
    "    (0,0,1),\n",
    "    (0,1,1),\n",
    "    (1,0,1),\n",
    "    (1,1,0)\n",
    "]\n",
    "\n",
    "NOR = [\n",
    "    (0,0,1),\n",
    "    (0,1,0),\n",
    "    (1,0,0),\n",
    "    (1,1,0)\n",
    "]\n",
    "\n",
    "XOR = [\n",
    "    (0,0,0),\n",
    "    (0,1,1),\n",
    "    (1,0,1),\n",
    "    (1,1,0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730f2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, lr, epochs):\n",
    "    w = [0, 0]\n",
    "    b = 0\n",
    "    for epoch in range(epochs):\n",
    "        for x1, x2, y in dataset:\n",
    "            z = w[0]*x1 + w[1]*x2 + b\n",
    "            y_hat = 1 if z > 0 else 0\n",
    "            error = y - y_hat\n",
    "            w[0] += lr * error * x1\n",
    "            w[1] += lr * error * x2\n",
    "            b += lr * error\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9967e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, w, b):\n",
    "    for x1, x2, y in dataset:\n",
    "        z = w[0]*x1 + w[1]*x2 + b\n",
    "        y_hat = 1 if z > 0 else 0\n",
    "        print(f\"Input: ({x1}, {x2}), Predicted: {y_hat}, Actual: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845777df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on AND gate\n",
      "Final weights: [0.2, 0.1], bias: -0.2\n",
      "Input: (0, 0), Predicted: 0, Actual: 0\n",
      "Input: (0, 1), Predicted: 0, Actual: 0\n",
      "Input: (1, 0), Predicted: 0, Actual: 0\n",
      "Input: (1, 1), Predicted: 1, Actual: 1\n",
      "\n",
      "Training on OR gate\n",
      "Final weights: [0.1, 0.1], bias: 0.0\n",
      "Input: (0, 0), Predicted: 0, Actual: 0\n",
      "Input: (0, 1), Predicted: 1, Actual: 1\n",
      "Input: (1, 0), Predicted: 1, Actual: 1\n",
      "Input: (1, 1), Predicted: 1, Actual: 1\n",
      "\n",
      "Training on NAND gate\n",
      "Final weights: [-0.2, -0.1], bias: 0.20000000000000004\n",
      "Input: (0, 0), Predicted: 1, Actual: 1\n",
      "Input: (0, 1), Predicted: 1, Actual: 1\n",
      "Input: (1, 0), Predicted: 1, Actual: 1\n",
      "Input: (1, 1), Predicted: 0, Actual: 0\n",
      "\n",
      "Training on NOR gate\n",
      "Final weights: [-0.1, -0.1], bias: 0.1\n",
      "Input: (0, 0), Predicted: 1, Actual: 1\n",
      "Input: (0, 1), Predicted: 0, Actual: 0\n",
      "Input: (1, 0), Predicted: 0, Actual: 0\n",
      "Input: (1, 1), Predicted: 0, Actual: 0\n",
      "\n",
      "Training on XOR gate\n",
      "Final weights: [-0.1, 0.0], bias: 0.1\n",
      "Input: (0, 0), Predicted: 1, Actual: 0\n",
      "Input: (0, 1), Predicted: 1, Actual: 1\n",
      "Input: (1, 0), Predicted: 0, Actual: 1\n",
      "Input: (1, 1), Predicted: 0, Actual: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gates = {\n",
    "    \"AND\": AND,\n",
    "    \"OR\": OR,\n",
    "    \"NAND\": NAND,\n",
    "    \"NOR\": NOR,\n",
    "    \"XOR\": XOR\n",
    "}\n",
    "for gate in gates:\n",
    "    print(f\"Training on {gate} gate\")\n",
    "    data = gates[gate]\n",
    "    w, b = train(data, lr=0.1, epochs=10)\n",
    "    print(f\"Final weights: {w}, bias: {b}\")\n",
    "    test(data, w, b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace3a45",
   "metadata": {},
   "source": [
    "**Effect of learning rate (Î·):**\n",
    "\n",
    "- A smaller learning rate (e.g., 0.01) makes weight updates small. Training is more stable but may need more epochs to reach correct weights.\n",
    "\n",
    "\n",
    "- A larger learning rate (e.g., 0.5 or 1.0) makes updates large. It can learn faster when it works, but it may overshoot the solution and oscillate or fail to converge for some datasets.\n",
    "\n",
    "In our code, changing `lr` mainly changes how fast the perceptron reaches a set of weights that correctly classifies all linearly separable gates (AND, OR, NAND, NOR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581edf87",
   "metadata": {},
   "source": [
    "**Why the same code learned different gates:**\n",
    "\n",
    "- The perceptron learning rule is the same, but the training data (truth table) is different for each gate.\n",
    "- For each gate, the algorithm adjusts weights and bias until it finds a linear decision boundary that separates outputs 0 and 1 for that truth table.\n",
    "- AND, OR, NAND, and NOR are **linearly separable**, so a single-layer perceptron can find suitable weights and bias for each of them.\n",
    "- XOR is **not linearly separable**. No single straight line can separate its 0 and 1 outputs, so a single-layer perceptron cannot learn XOR perfectly, no matter how you tune the learning rate or epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
